{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ***Installing and Importing***"
      ],
      "metadata": {
        "id": "zECIUCuvre38"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3LPZKOf3rKYF",
        "outputId": "14748c3e-88db-4b3e-fec9-e77d619a8c5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "transformers 4.57.3 requires tokenizers<=0.23.0,>=0.22.0, but you have tokenizers 0.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "chromadb 0.5.23 requires tokenizers<=0.20.3,>=0.13.2, but you have tokenizers 0.22.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m All packages installed successfully!\n"
          ]
        }
      ],
      "source": [
        "# Core packages for document processing\n",
        "!pip install -q docling\n",
        "!pip install -q gradio\n",
        "\n",
        "# Vector store and embeddings\n",
        "!pip install -q chromadb==0.5.23\n",
        "!pip install -q sentence-transformers\n",
        "\n",
        "# LLM integration\n",
        "!pip install -q google-generativeai\n",
        "\n",
        "# Scientific computing & utilities\n",
        "!pip install -q scikit-learn\n",
        "!pip install -q pandas\n",
        "!pip install -q numpy\n",
        "!pip install -q tqdm\n",
        "\n",
        "print(\" All packages installed successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CORE IMPORTS AND CONFIGURATION\n",
        "import os\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "from dataclasses import dataclass, field\n",
        "from typing import List, Dict, Tuple, Optional, Any\n",
        "import json\n",
        "\n",
        "# Suppress warnings for cleaner output\n",
        "warnings.filterwarnings('ignore')\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "\n",
        "# Document Processing\n",
        "from docling.document_converter import DocumentConverter\n",
        "from docling.datamodel.base_models import InputFormat\n",
        "from docling.datamodel.pipeline_options import PdfPipelineOptions\n",
        "from docling.backend.pypdfium2_backend import PyPdfiumDocumentBackend\n",
        "\n",
        "# Machine Learning & Embeddings\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Vector Store\n",
        "import chromadb\n",
        "from chromadb.config import Settings\n",
        "\n",
        "# LLM Integration\n",
        "import google.generativeai as genai\n",
        "\n",
        "# User Interface\n",
        "import gradio as gr\n",
        "\n",
        "# Utilities\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"All imports successful!\")\n",
        "print(f\"System initialized at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "deiick43rt3k",
        "outputId": "0692db26-1911-4b3e-b991-b3cdf8b49a54"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All imports successful!\n",
            "System initialized at: 2026-01-05 08:42:46\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Data Models & Configuration***"
      ],
      "metadata": {
        "id": "p9sfm4dptfYe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration Class\n",
        "@dataclass\n",
        "class RAGConfig:\n",
        "    # Embedding Model (runs on CPU, saves GPU)\n",
        "    embedding_model: str = \"all-MiniLM-L6-v2\"  # Fast, 384 dimensions, 22M params\n",
        "\n",
        "    # Chunking Strategy\n",
        "    chunk_size: int = 800              # Optimal for context preservation\n",
        "    chunk_overlap: int = 150           # Ensures continuity between chunks\n",
        "\n",
        "    # Retrieval Settings\n",
        "    top_k_vector: int = 5              # Top vector search results\n",
        "    top_k_knn: int = 3                 # Top KNN results for hybrid search\n",
        "    similarity_threshold: float = 0.3  # Minimum relevance score\n",
        "\n",
        "    # LLM Settings\n",
        "    llm_model: str = \"gemini-2.0-flash-exp\"  # Latest experimental model\n",
        "    llm_temperature: float = 0.3       # Lower = more focused responses\n",
        "    llm_max_tokens: int = 2048         # Response length limit\n",
        "\n",
        "    # Document Processing\n",
        "    max_pdf_pages: int = 100           # Safety limit for large PDFs\n",
        "\n",
        "    # Vector Store\n",
        "    collection_name: str = \"documents_collection\"\n",
        "    persist_directory: str = \"./chroma_db\"\n",
        "\n",
        "\n",
        "# Document Chunk Model\n",
        "@dataclass\n",
        "class DocumentChunk:\n",
        "    chunk_id: str                      # Unique identifier\n",
        "    text: str                          # Actual content\n",
        "    page_number: int                   # Source page\n",
        "    chunk_index: int                   # Position in document\n",
        "    doc_type: str                      # e.g., \"resume\", \"invoice\", \"contract\"\n",
        "    metadata: Dict[str, Any] = field(default_factory=dict)  # Flexible metadata\n",
        "    embedding: Optional[np.ndarray] = None  # Vector representation\n",
        "\n",
        "    def to_dict(self) -> Dict[str, Any]:\n",
        "        \"\"\"Convert to dictionary for storage\"\"\"\n",
        "        return {\n",
        "            'chunk_id': self.chunk_id,\n",
        "            'text': self.text,\n",
        "            'page_number': self.page_number,\n",
        "            'chunk_index': self.chunk_index,\n",
        "            'doc_type': self.doc_type,\n",
        "            'metadata': self.metadata\n",
        "        }\n",
        "\n",
        "# Retrieval Result Model\n",
        "@dataclass\n",
        "class RetrievalResult:\n",
        "    chunk: DocumentChunk\n",
        "    relevance_score: float             # Similarity score (0-1)\n",
        "    retrieval_method: str              # \"vector\" or \"knn\" or \"hybrid\"\n",
        "    rank: int                          # Position in results (1-indexed)\n",
        "\n",
        "    def __str__(self) -> str:\n",
        "        return (f\"Rank {self.rank} | Score: {self.relevance_score:.3f} | \"\n",
        "                f\"Method: {self.retrieval_method} | Page: {self.chunk.page_number}\")\n",
        "\n",
        "\n",
        "# Initialize Global Configuration\n",
        "config = RAGConfig()\n",
        "\n",
        "print(\"Data models and configuration initialized!\")\n",
        "print(\"-\" * 50)\n",
        "print(\" Configuration Summary:\")\n",
        "print(f\"Embedding Model: {config.embedding_model}\")\n",
        "print(f\"Chunk Size: {config.chunk_size} characters\")\n",
        "print(f\"Top-K Results: {config.top_k_vector} (vector) + {config.top_k_knn} (KNN)\")\n",
        "print(f\"LLM Model: {config.llm_model}\")\n",
        "print(f\"Temperature: {config.llm_temperature}\")\n",
        "print(\"-\" * 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jS2mcOQAs-hs",
        "outputId": "7c8a009b-cd48-4076-9092-6c9a280f7cf9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data models and configuration initialized!\n",
            "--------------------------------------------------\n",
            " Configuration Summary:\n",
            "Embedding Model: all-MiniLM-L6-v2\n",
            "Chunk Size: 800 characters\n",
            "Top-K Results: 5 (vector) + 3 (KNN)\n",
            "LLM Model: gemini-2.0-flash-exp\n",
            "Temperature: 0.3\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting up Gemini API and initializing embedding model + vector store\n",
        "\n",
        "from google.colab import userdata\n",
        "import time\n",
        "from google.generativeai.types import HarmCategory, HarmBlockThreshold\n",
        "\n",
        "print(\"Configuring API access...\")\n",
        "\n",
        "\n",
        "# Gemini API Setup\n",
        "GEMINI_API_KEY = \"AIzaSyChTSFOWOm3-DYsm10e0aVjVpr3_a-wY1o\"\n",
        "genai.configure(api_key=GEMINI_API_KEY)\n",
        "config.llm_model = \"gemini-2.5-flash\"\n",
        "print(f\"Gemini configured with model: {config.llm_model}\")\n",
        "\n",
        "# Initialize Embedding Model\n",
        "print(\"\\nLoading embedding model...\")\n",
        "start_time = time.time()\n",
        "\n",
        "# Check if already loaded\n",
        "try:\n",
        "    _ = embedding_model.get_sentence_embedding_dimension()\n",
        "    print(\"Embedding model already loaded (reusing)\")\n",
        "except:\n",
        "    embedding_model = SentenceTransformer(config.embedding_model)\n",
        "    embedding_model = embedding_model.to('cpu')\n",
        "    print(f\"Embedding model loaded in {time.time() - start_time:.2f} seconds!\")\n",
        "\n",
        "print(f\"Model: {config.embedding_model}\")\n",
        "print(f\"Embedding dimension: {embedding_model.get_sentence_embedding_dimension()}\")\n",
        "\n",
        "#  Initialize ChromaDB Vector Store\n",
        "print(\"\\nInitializing vector database...\")\n",
        "\n",
        "# Create persistent directory\n",
        "Path(config.persist_directory).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Initialize ChromaDB client\n",
        "chroma_client = chromadb.PersistentClient(\n",
        "    path=config.persist_directory,\n",
        "    settings=Settings(\n",
        "        anonymized_telemetry=False,\n",
        "        allow_reset=True\n",
        "    )\n",
        ")\n",
        "\n",
        "# Create fresh collection\n",
        "try:\n",
        "    try:\n",
        "        chroma_client.delete_collection(name=config.collection_name)\n",
        "        print(\"Cleared existing collection\")\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    collection = chroma_client.create_collection(\n",
        "        name=config.collection_name,\n",
        "        metadata={\"hnsw:space\": \"cosine\"}\n",
        "    )\n",
        "    print(f\"Vector database ready: '{config.collection_name}'\")\n",
        "\n",
        "except Exception as e:\n",
        "    collection = chroma_client.get_collection(name=config.collection_name)\n",
        "    print(\"Using existing collection\")\n",
        "\n",
        "# Initialize Gemini LLM (Correct safety settings format)\n",
        "print(\"\\nInitializing Gemini LLM...\")\n",
        "\n",
        "llm = genai.GenerativeModel(\n",
        "    model_name=config.llm_model,\n",
        "    generation_config={\n",
        "        \"temperature\": config.llm_temperature,\n",
        "        \"max_output_tokens\": config.llm_max_tokens,\n",
        "        \"top_p\": 0.95,\n",
        "        \"top_k\": 40,\n",
        "    },\n",
        "    safety_settings={\n",
        "        HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
        "        HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
        "        HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
        "        HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n",
        "    }\n",
        ")\n",
        "\n",
        "print(f\"LLM ready: {config.llm_model}\")\n",
        "print(f\"Free tier: 15 RPM, 1M TPM, 1500 RPD\")\n",
        "\n",
        "print(\"ALL SYSTEMS OPERATIONAL!\")\n",
        "print(f\"System Status:\")\n",
        "print(f\"Embedding Model: {config.embedding_model} (CPU)\")\n",
        "print(f\"Vector Database: {collection.count()} documents stored\")\n",
        "print(f\"LLM: {config.llm_model} (Ready)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-4JqQVOuTCb",
        "outputId": "ac33965b-0e41-4aaa-b1e4-64514af6c560"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuring API access...\n",
            "Gemini configured with model: gemini-2.5-flash\n",
            "\n",
            "Loading embedding model...\n",
            "Embedding model loaded in 5.05 seconds!\n",
            "Model: all-MiniLM-L6-v2\n",
            "Embedding dimension: 384\n",
            "\n",
            "Initializing vector database...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
            "ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleared existing collection\n",
            "Vector database ready: 'documents_collection'\n",
            "\n",
            "Initializing Gemini LLM...\n",
            "LLM ready: gemini-2.5-flash\n",
            "Free tier: 15 RPM, 1M TPM, 1500 RPD\n",
            "ALL SYSTEMS OPERATIONAL!\n",
            "System Status:\n",
            "Embedding Model: all-MiniLM-L6-v2 (CPU)\n",
            "Vector Database: 0 documents stored\n",
            "LLM: gemini-2.5-flash (Ready)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Advanced PDF Processing with Docling***"
      ],
      "metadata": {
        "id": "wghtzAbHxsOg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import hashlib\n",
        "from typing import List, Dict, Tuple\n",
        "from collections import Counter\n",
        "\n",
        "def classify_document_type(text: str, filename: str = \"\") -> str:\n",
        "    \"\"\"\n",
        "    Intelligently classify document type based on content and filename.\n",
        "    \"\"\"\n",
        "    text_lower = text.lower()[:2000]\n",
        "    filename_lower = filename.lower()\n",
        "\n",
        "    rules = [\n",
        "        (['resume', 'cv', 'curriculum'],\n",
        "         ['experience', 'education', 'skills', 'employment', 'objective', 'career summary'],\n",
        "         'resume'),\n",
        "        (['invoice', 'bill', 'receipt', 'fee', 'worksheet'],\n",
        "         ['invoice', 'total', 'amount', 'payment', 'due date', 'subtotal', 'purchase price', 'closing'],\n",
        "         'invoice'),\n",
        "        (['payslip', 'salary', 'paystub'],\n",
        "         ['gross pay', 'net pay', 'deductions', 'salary', 'earnings', 'employee id', 'working days'],\n",
        "         'payslip'),\n",
        "        (['contract', 'agreement', 'terms'],\n",
        "         ['party', 'agreement', 'terms and conditions', 'hereby', 'whereas'],\n",
        "         'contract'),\n",
        "        (['balance', 'statement', 'financial'],\n",
        "         ['assets', 'liabilities', 'equity', 'balance sheet', 'fiscal'],\n",
        "         'financial_statement'),\n",
        "        (['report', 'analysis', 'study'],\n",
        "         ['executive summary', 'findings', 'conclusion', 'methodology'],\n",
        "         'report'),\n",
        "    ]\n",
        "\n",
        "    for filename_keywords, content_keywords, doc_type in rules:\n",
        "        filename_match = any(kw in filename_lower for kw in filename_keywords)\n",
        "        content_matches = sum(1 for kw in content_keywords if kw in text_lower)\n",
        "\n",
        "        # Lower threshold for better detection\n",
        "        if filename_match or content_matches >= 2:\n",
        "            return doc_type\n",
        "\n",
        "    return 'general'\n",
        "\n",
        "\n",
        "# Extract and Process PDF with Docling\n",
        "def extract_pdf_with_docling(pdf_path: str) -> Tuple[str, List[Dict], str]:\n",
        "    try:\n",
        "        print(f\"Processing PDF with Docling: {Path(pdf_path).name}\")\n",
        "\n",
        "        converter = DocumentConverter(allowed_formats=[InputFormat.PDF])\n",
        "        print(\"Converting document...\")\n",
        "        result = converter.convert(pdf_path)\n",
        "\n",
        "        # Get full text first\n",
        "        full_text = \"\"\n",
        "        try:\n",
        "            full_text = result.document.export_to_markdown()\n",
        "            print(\"Exported as Markdown\")\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        if not full_text:\n",
        "            try:\n",
        "                if hasattr(result.document, 'text'):\n",
        "                    full_text = result.document.text\n",
        "                elif hasattr(result.document, 'content'):\n",
        "                    full_text = result.document.content\n",
        "                print(\"Exported as Text\")\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        if not full_text:\n",
        "            try:\n",
        "                page_texts = []\n",
        "                for page in result.document.pages:\n",
        "                    if hasattr(page, 'export_to_markdown'):\n",
        "                        page_texts.append(page.export_to_markdown())\n",
        "                    elif hasattr(page, 'text'):\n",
        "                        page_texts.append(page.text)\n",
        "                    elif hasattr(page, 'content'):\n",
        "                        page_texts.append(page.content)\n",
        "                full_text = \"\\n\\n\".join(page_texts)\n",
        "                print(\"Extracted from pages\")\n",
        "            except Exception as e:\n",
        "                print(f\"⚠️  Page extraction error: {e}\")\n",
        "\n",
        "        if not full_text or len(full_text.strip()) < 50:\n",
        "            raise ValueError(\"Could not extract meaningful text from PDF\")\n",
        "\n",
        "        pages_info = []\n",
        "        try:\n",
        "            if hasattr(result.document, 'pages') and len(result.document.pages) > 0:\n",
        "                for page_num, page in enumerate(result.document.pages, start=1):\n",
        "                    page_text = \"\"\n",
        "                    try:\n",
        "                        page_text = page.export_to_markdown()\n",
        "                    except:\n",
        "                        pass\n",
        "\n",
        "                    if not page_text:\n",
        "                        try:\n",
        "                            page_text = page.text if hasattr(page, 'text') else \"\"\n",
        "                        except:\n",
        "                            pass\n",
        "\n",
        "                    if not page_text:\n",
        "                        try:\n",
        "                            page_text = page.content if hasattr(page, 'content') else \"\"\n",
        "                        except:\n",
        "                            pass\n",
        "\n",
        "                    if page_text and len(page_text.strip()) > 20:\n",
        "                        page_doc_type = classify_document_type(page_text, Path(pdf_path).name)\n",
        "\n",
        "                        pages_info.append({\n",
        "                            'page_number': page_num,\n",
        "                            'text': page_text,\n",
        "                            'char_count': len(page_text),\n",
        "                            'doc_type': page_doc_type,\n",
        "                            'has_tables': False,\n",
        "                            'has_images': False\n",
        "                        })\n",
        "        except Exception as e:\n",
        "            print(f\"Could not extract individual pages: {e}\")\n",
        "\n",
        "        # Fallback: Split full text intelligently if page extraction failed\n",
        "        if not pages_info or all(p['char_count'] < 30 for p in pages_info):\n",
        "            print(\"Per-page extraction failed, splitting full text...\")\n",
        "\n",
        "            total_chars = len(full_text)\n",
        "            num_pages = len(result.document.pages) if hasattr(result.document, 'pages') else 1\n",
        "            chars_per_page = total_chars // num_pages if num_pages > 0 else total_chars\n",
        "\n",
        "            pages_info = []\n",
        "            start = 0\n",
        "\n",
        "            for page_num in range(1, num_pages + 1):\n",
        "                if page_num == num_pages:\n",
        "                    end = total_chars\n",
        "                else:\n",
        "                    target_end = start + chars_per_page\n",
        "                    search_start = max(start, target_end - 100)\n",
        "                    search_end = min(total_chars, target_end + 100)\n",
        "\n",
        "                    para_break = full_text.find('\\n\\n', search_start, search_end)\n",
        "                    if para_break > start:\n",
        "                        end = para_break + 2\n",
        "                    else:\n",
        "                        end = target_end\n",
        "\n",
        "                page_text = full_text[start:end].strip()\n",
        "\n",
        "                if page_text:\n",
        "                    page_doc_type = classify_document_type(page_text, Path(pdf_path).name)\n",
        "\n",
        "                    pages_info.append({\n",
        "                        'page_number': page_num,\n",
        "                        'text': page_text,\n",
        "                        'char_count': len(page_text),\n",
        "                        'doc_type': page_doc_type,\n",
        "                        'has_tables': False,\n",
        "                        'has_images': False\n",
        "                    })\n",
        "\n",
        "                start = end\n",
        "\n",
        "        # Final validation\n",
        "        if not pages_info:\n",
        "            overall_type = classify_document_type(full_text, Path(pdf_path).name)\n",
        "            pages_info = [{\n",
        "                'page_number': 1,\n",
        "                'text': full_text,\n",
        "                'char_count': len(full_text),\n",
        "                'doc_type': overall_type,\n",
        "                'has_tables': False,\n",
        "                'has_images': False\n",
        "            }]\n",
        "        page_types = [p['doc_type'] for p in pages_info]\n",
        "        type_counts = Counter(page_types)\n",
        "        overall_doc_type = type_counts.most_common(1)[0][0]\n",
        "\n",
        "        print(f\"Extracted {len(pages_info)} pages\")\n",
        "        print(f\"Total characters: {len(full_text):,}\")\n",
        "        print(f\"Page sizes: {[p['char_count'] for p in pages_info]}\")\n",
        "        print(f\"Per-page types: {page_types}\")\n",
        "        print(f\"Overall type: {overall_doc_type} (most common)\")\n",
        "\n",
        "        return full_text, pages_info, overall_doc_type\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Docling failed: {e}\")\n",
        "        print(f\"Falling back to PyPDF2...\")\n",
        "\n",
        "        try:\n",
        "            import PyPDF2\n",
        "            with open(pdf_path, 'rb') as file:\n",
        "                pdf_reader = PyPDF2.PdfReader(file)\n",
        "                pages_info = []\n",
        "                page_texts = []\n",
        "\n",
        "                for page_num in range(len(pdf_reader.pages)):\n",
        "                    page_text = pdf_reader.pages[page_num].extract_text()\n",
        "                    page_doc_type = classify_document_type(page_text, Path(pdf_path).name)\n",
        "\n",
        "                    page_texts.append(page_text)\n",
        "                    pages_info.append({\n",
        "                        'page_number': page_num + 1,\n",
        "                        'text': page_text,\n",
        "                        'char_count': len(page_text),\n",
        "                        'doc_type': page_doc_type,\n",
        "                        'has_tables': False,\n",
        "                        'has_images': False\n",
        "                    })\n",
        "\n",
        "                full_text = \"\\n\\n\".join(page_texts)\n",
        "\n",
        "                # Overall type\n",
        "                page_types = [p['doc_type'] for p in pages_info]\n",
        "                overall_doc_type = Counter(page_types).most_common(1)[0][0]\n",
        "\n",
        "                print(f\"PyPDF2 fallback successful!\")\n",
        "                print(f\"Per-page types: {page_types}\")\n",
        "                print(f\"Overall type: {overall_doc_type}\")\n",
        "\n",
        "                return full_text, pages_info, overall_doc_type\n",
        "\n",
        "        except Exception as fallback_error:\n",
        "            raise Exception(f\"Both Docling and PyPDF2 failed: {fallback_error}\")\n",
        "\n",
        "\n",
        "def chunk_text_with_metadata(\n",
        "    text: str,\n",
        "    page_number: int,\n",
        "    doc_type: str,\n",
        "    chunk_size: int = 800,\n",
        "    chunk_overlap: int = 150\n",
        ") -> List[DocumentChunk]:\n",
        "    chunks = []\n",
        "\n",
        "    # Clean text\n",
        "    text = text.strip()\n",
        "    text = '\\n'.join(line.strip() for line in text.split('\\n') if line.strip())\n",
        "\n",
        "    # Validate text\n",
        "    if not text or len(text) < 30:\n",
        "        print(f\"Page {page_number}: text too short ({len(text)} chars), skipping\")\n",
        "        return chunks\n",
        "\n",
        "    # Single chunk if small enough\n",
        "    if len(text) <= chunk_size:\n",
        "        chunk_id = hashlib.md5(\n",
        "            f\"{doc_type}_{page_number}_0_{text[:50]}\".encode()\n",
        "        ).hexdigest()[:16]\n",
        "\n",
        "        chunks.append(DocumentChunk(\n",
        "            chunk_id=chunk_id,\n",
        "            text=text,\n",
        "            page_number=page_number,\n",
        "            chunk_index=0,\n",
        "            doc_type=doc_type,\n",
        "            metadata={'char_count': len(text)}\n",
        "        ))\n",
        "        print(f\"Page {page_number} ({doc_type}): 1 chunk ({len(text)} chars)\")\n",
        "        return chunks\n",
        "\n",
        "    start = 0\n",
        "    chunk_index = 0\n",
        "\n",
        "    while start < len(text):\n",
        "        end = start + chunk_size\n",
        "\n",
        "        if end < len(text):\n",
        "            search_start = max(start, end - 100)\n",
        "\n",
        "            last_para = text.rfind('\\n\\n', search_start, end)\n",
        "            if last_para > start:\n",
        "                end = last_para + 2\n",
        "            else:\n",
        "                last_period = text.rfind('. ', search_start, end)\n",
        "                if last_period > start:\n",
        "                    end = last_period + 2\n",
        "                else:\n",
        "                    last_space = text.rfind(' ', search_start, end)\n",
        "                    if last_space > start:\n",
        "                        end = last_space + 1\n",
        "\n",
        "        chunk_text = text[start:end].strip()\n",
        "\n",
        "        if len(chunk_text) >= 30:\n",
        "            chunk_id = hashlib.md5(\n",
        "                f\"{doc_type}_{page_number}_{chunk_index}_{chunk_text[:50]}\".encode()\n",
        "            ).hexdigest()[:16]\n",
        "\n",
        "            chunks.append(DocumentChunk(\n",
        "                chunk_id=chunk_id,\n",
        "                text=chunk_text,\n",
        "                page_number=page_number,\n",
        "                chunk_index=chunk_index,\n",
        "                doc_type=doc_type,\n",
        "                metadata={'char_count': len(chunk_text)}\n",
        "            ))\n",
        "            chunk_index += 1\n",
        "\n",
        "        if end >= len(text):\n",
        "            break\n",
        "\n",
        "        start = end - chunk_overlap\n",
        "\n",
        "        if start <= end - chunk_size:\n",
        "            start = end - chunk_overlap\n",
        "\n",
        "    print(f\"Page {page_number} ({doc_type}): {len(chunks)} chunks ({len(text)} chars)\")\n",
        "    return chunks\n",
        "\n",
        "\n",
        "print(\"PDF processing functions initialized!\")\n",
        "print(\"Available functions:\")\n",
        "print(\"• classify_document_type() - Smart document classification\")\n",
        "print(\"• extract_pdf_with_docling() - FIXED with per-page type detection\")\n",
        "print(\"• chunk_text_with_metadata() - Uses per-page doc types\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fxvNunuvdEq",
        "outputId": "6812e9b1-3170-4cc2-eae8-88f0a5a7c3a0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PDF processing functions initialized!\n",
            "Available functions:\n",
            "• classify_document_type() - Smart document classification\n",
            "• extract_pdf_with_docling() - FIXED with per-page type detection\n",
            "• chunk_text_with_metadata() - Uses per-page doc types\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Hybrid Retrieval System (Vector + KNN)***"
      ],
      "metadata": {
        "id": "Tf4gMI3yx3FP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combines dense vector search (semantic) with KNN (diverse results)\n",
        "# This gives us BOTH accuracy AND diversity in retrieval\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# Document Ingestion Pipeline\n",
        "def ingest_document(pdf_path: str) -> Dict[str, Any]:\n",
        "    try:\n",
        "        print(f\"INGESTING DOCUMENT: {Path(pdf_path).name}\")\n",
        "        # Step 1: Extract text\n",
        "        full_text, pages_info, doc_type = extract_pdf_with_docling(pdf_path)\n",
        "\n",
        "        # Step 2: Chunk each page\n",
        "        all_chunks = []\n",
        "        print(f\"\\nChunking {len(pages_info)} pages...\")\n",
        "\n",
        "        for page_info in pages_info:\n",
        "            page_chunks = chunk_text_with_metadata(\n",
        "                text=page_info['text'],\n",
        "                page_number=page_info['page_number'],\n",
        "                doc_type=doc_type,\n",
        "                chunk_size=config.chunk_size,\n",
        "                chunk_overlap=config.chunk_overlap\n",
        "            )\n",
        "            all_chunks.extend(page_chunks)\n",
        "            if page_chunks:\n",
        "                print(f\"Page {page_info['page_number']}: {len(page_chunks)} chunks\")\n",
        "\n",
        "        # Validate we have chunks\n",
        "        if not all_chunks:\n",
        "            raise ValueError(\"No chunks created. PDF may be empty or image-based.\")\n",
        "\n",
        "        print(f\"\\nCreated {len(all_chunks)} total chunks\")\n",
        "\n",
        "        # Step 3: Generate embeddings\n",
        "        print(\"Generating embeddings...\")\n",
        "        texts = [chunk.text for chunk in all_chunks]\n",
        "\n",
        "        # Validate texts\n",
        "        texts = [t for t in texts if t and len(t.strip()) > 0]\n",
        "        if not texts:\n",
        "            raise ValueError(\"No valid text for embedding\")\n",
        "\n",
        "        embeddings = embedding_model.encode(\n",
        "            texts,\n",
        "            show_progress_bar=True,\n",
        "            batch_size=32,\n",
        "            convert_to_numpy=True\n",
        "        )\n",
        "\n",
        "        print(f\"Generated {len(embeddings)} embeddings\")\n",
        "\n",
        "        # Add embeddings to chunks\n",
        "        for chunk, embedding in zip(all_chunks, embeddings):\n",
        "            chunk.embedding = embedding\n",
        "\n",
        "        # Step 4: Store in ChromaDB\n",
        "        print(\"Storing in vector database...\")\n",
        "        collection.add(\n",
        "            ids=[chunk.chunk_id for chunk in all_chunks],\n",
        "            embeddings=embeddings.tolist(),\n",
        "            documents=texts,\n",
        "            metadatas=[{\n",
        "                'page_number': chunk.page_number,\n",
        "                'chunk_index': chunk.chunk_index,\n",
        "                'doc_type': chunk.doc_type,\n",
        "                'char_count': len(chunk.text)\n",
        "            } for chunk in all_chunks]\n",
        "        )\n",
        "\n",
        "        stats = {\n",
        "            'filename': Path(pdf_path).name,\n",
        "            'doc_type': doc_type,\n",
        "            'pages': len(pages_info),\n",
        "            'chunks': len(all_chunks),\n",
        "            'total_chars': len(full_text),\n",
        "            'avg_chunk_size': len(full_text) // len(all_chunks) if all_chunks else 0\n",
        "        }\n",
        "\n",
        "        print(f\"\\nDocument ingested successfully!\")\n",
        "        print(f\"Total chunks in database: {collection.count()}\")\n",
        "\n",
        "        return stats\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error ingesting document: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        raise\n",
        "\n",
        "\n",
        "# Vector Search (Semantic Similarity)\n",
        "def vector_search(query: str, top_k: int = 5) -> List[RetrievalResult]:\n",
        "\n",
        "    # Generate query embedding\n",
        "    query_embedding = embedding_model.encode([query], convert_to_numpy=True)[0]\n",
        "\n",
        "    # Search in ChromaDB\n",
        "    results = collection.query(\n",
        "        query_embeddings=[query_embedding.tolist()],\n",
        "        n_results=min(top_k, collection.count()),\n",
        "        include=['documents', 'metadatas', 'distances']\n",
        "    )\n",
        "\n",
        "    # Convert to RetrievalResult objects\n",
        "    retrieval_results = []\n",
        "    for idx, (doc, metadata, distance) in enumerate(zip(\n",
        "        results['documents'][0],\n",
        "        results['metadatas'][0],\n",
        "        results['distances'][0]\n",
        "    )):\n",
        "        # Convert distance to similarity score (ChromaDB returns L2 distance)\n",
        "        # For cosine distance: similarity = 1 - (distance / 2)\n",
        "        similarity = 1 - (distance / 2)\n",
        "\n",
        "        # Create DocumentChunk from stored data\n",
        "        chunk = DocumentChunk(\n",
        "            chunk_id=results['ids'][0][idx],\n",
        "            text=doc,\n",
        "            page_number=metadata['page_number'],\n",
        "            chunk_index=metadata['chunk_index'],\n",
        "            doc_type=metadata['doc_type'],\n",
        "            metadata=metadata\n",
        "        )\n",
        "\n",
        "        retrieval_results.append(RetrievalResult(\n",
        "            chunk=chunk,\n",
        "            relevance_score=similarity,\n",
        "            retrieval_method='vector',\n",
        "            rank=idx + 1\n",
        "        ))\n",
        "\n",
        "    return retrieval_results\n",
        "\n",
        "\n",
        "#  KNN Search (Diversity-based Retrieval)\n",
        "def knn_search(query: str, top_k: int = 3, all_chunks: List[str] = None) -> List[RetrievalResult]:\n",
        "    # Fetch all chunks if not provided\n",
        "    if all_chunks is None:\n",
        "        all_results = collection.get(include=['documents', 'metadatas'])\n",
        "        all_chunks_data = all_results['documents']\n",
        "        all_metadata = all_results['metadatas']\n",
        "        all_ids = all_results['ids']\n",
        "    else:\n",
        "        all_chunks_data = all_chunks\n",
        "\n",
        "    if not all_chunks_data:\n",
        "        return []\n",
        "\n",
        "    # Create TF-IDF vectors\n",
        "    vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n",
        "\n",
        "    # Fit on all chunks + query\n",
        "    all_texts = all_chunks_data + [query]\n",
        "    tfidf_matrix = vectorizer.fit_transform(all_texts)\n",
        "\n",
        "    # Query vector is the last one\n",
        "    query_vector = tfidf_matrix[-1]\n",
        "    chunk_vectors = tfidf_matrix[:-1]\n",
        "\n",
        "    # Calculate cosine similarity\n",
        "    similarities = cosine_similarity(query_vector, chunk_vectors).flatten()\n",
        "\n",
        "    # Get top-k indices\n",
        "    top_indices = similarities.argsort()[-top_k:][::-1]\n",
        "\n",
        "    # Convert to RetrievalResult objects\n",
        "    retrieval_results = []\n",
        "    for rank, idx in enumerate(top_indices, start=1):\n",
        "        chunk = DocumentChunk(\n",
        "            chunk_id=all_ids[idx],\n",
        "            text=all_chunks_data[idx],\n",
        "            page_number=all_metadata[idx]['page_number'],\n",
        "            chunk_index=all_metadata[idx]['chunk_index'],\n",
        "            doc_type=all_metadata[idx]['doc_type'],\n",
        "            metadata=all_metadata[idx]\n",
        "        )\n",
        "\n",
        "        retrieval_results.append(RetrievalResult(\n",
        "            chunk=chunk,\n",
        "            relevance_score=float(similarities[idx]),\n",
        "            retrieval_method='knn',\n",
        "            rank=rank\n",
        "        ))\n",
        "\n",
        "    return retrieval_results\n",
        "\n",
        "# Hybrid Search (Best of Both Worlds)\n",
        "def hybrid_search(query: str, top_k_total: int = 8) -> List[RetrievalResult]:\n",
        "\n",
        "    print(f\"\\nPerforming hybrid search for: '{query[:60]}...'\")\n",
        "\n",
        "    # Get vector search results\n",
        "    vector_results = vector_search(query, top_k=config.top_k_vector)\n",
        "    print(f\"Vector search: {len(vector_results)} results\")\n",
        "\n",
        "    # Get KNN results\n",
        "    knn_results = knn_search(query, top_k=config.top_k_knn)\n",
        "    print(f\"KNN search: {len(knn_results)} results\")\n",
        "\n",
        "    # Combine results (remove duplicates based on chunk_id)\n",
        "    seen_ids = set()\n",
        "    combined_results = []\n",
        "\n",
        "    # Add vector results first (they're usually more relevant)\n",
        "    for result in vector_results:\n",
        "        if result.chunk.chunk_id not in seen_ids:\n",
        "            seen_ids.add(result.chunk.chunk_id)\n",
        "            combined_results.append(result)\n",
        "\n",
        "    # Add KNN results (for diversity)\n",
        "    for result in knn_results:\n",
        "        if result.chunk.chunk_id not in seen_ids:\n",
        "            seen_ids.add(result.chunk.chunk_id)\n",
        "            result.retrieval_method = 'hybrid'  # Mark as hybrid\n",
        "            combined_results.append(result)\n",
        "\n",
        "    # Re-rank by relevance score\n",
        "    combined_results.sort(key=lambda x: x.relevance_score, reverse=True)\n",
        "\n",
        "    # Update ranks\n",
        "    for rank, result in enumerate(combined_results[:top_k_total], start=1):\n",
        "        result.rank = rank\n",
        "\n",
        "    print(f\"Combined: {len(combined_results[:top_k_total])} unique results\")\n",
        "\n",
        "    return combined_results[:top_k_total]\n",
        "\n",
        "\n",
        "print(\"Hybrid retrieval system initialized!\")\n",
        "print(\"Available functions:\")\n",
        "print(\"ingest_document() - Full ingestion pipeline\")\n",
        "print(\"vector_search() - Semantic similarity search\")\n",
        "print(\"knn_search() - TF-IDF based diversity search\")\n",
        "print(\"hybrid_search() - Combined retrieval\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xmTM1s09xvD_",
        "outputId": "0ab7df21-fc4f-4eb9-870f-aa8bc66c4dca"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hybrid retrieval system initialized!\n",
            "Available functions:\n",
            "ingest_document() - Full ingestion pipeline\n",
            "vector_search() - Semantic similarity search\n",
            "knn_search() - TF-IDF based diversity search\n",
            "hybrid_search() - Combined retrieval\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Answer Generation with Context***"
      ],
      "metadata": {
        "id": "xCTgu4mWzNzk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from typing import Optional\n",
        "\n",
        "# Prompt Engineering for Better Answers\n",
        "def create_rag_prompt(query: str, contexts: List[RetrievalResult]) -> str:\n",
        "    # Build context section with citations\n",
        "    context_parts = []\n",
        "    for i, result in enumerate(contexts, start=1):\n",
        "        context_parts.append(\n",
        "            f\"[Source {i} - Page {result.chunk.page_number}, \"\n",
        "            f\"Relevance: {result.relevance_score:.2f}]\\n\"\n",
        "            f\"{result.chunk.text}\\n\"\n",
        "        )\n",
        "    context_text = \"\\n\".join(context_parts)\n",
        "\n",
        "    # Create the prompt\n",
        "    prompt = f\"\"\"You are a highly accurate document analysis assistant. Your task is to answer questions based ONLY on the provided context from the document.\n",
        "\n",
        "CONTEXT FROM DOCUMENT:\n",
        "{context_text}\n",
        "\n",
        "USER QUESTION:\n",
        "{query}\n",
        "\n",
        "INSTRUCTIONS:\n",
        "1. Answer the question using ONLY information from the context above\n",
        "2. If the answer is in the context, provide it clearly and concisely\n",
        "3. Include specific details like numbers, dates, names when available\n",
        "4. Cite the source number(s) in your answer (e.g., \"According to Source 1...\")\n",
        "5. If the context doesn't contain enough information to answer, say \"I cannot find this information in the provided document\"\n",
        "6. Do NOT make up or infer information that isn't explicitly stated\n",
        "7. Keep your answer focused and relevant to the question\n",
        "\n",
        "ANSWER:\"\"\"\n",
        "\n",
        "    return prompt\n",
        "\n",
        "# Generate Answer with Retry Logic\n",
        "def generate_answer(\n",
        "    query: str,\n",
        "    contexts: List[RetrievalResult],\n",
        "    max_retries: int = 3\n",
        ") -> Dict[str, Any]:\n",
        "    import time\n",
        "\n",
        "    # Create prompt\n",
        "    prompt = create_rag_prompt(query, contexts)\n",
        "\n",
        "    # Attempt generation with retries\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            response = llm.generate_content(prompt)\n",
        "\n",
        "            # Check if response was blocked\n",
        "            if not response.text:\n",
        "                if response.prompt_feedback.block_reason:\n",
        "                    return {\n",
        "                        'answer': \"Response was blocked due to safety filters. Please rephrase your question.\",\n",
        "                        'sources': [],\n",
        "                        'error': f\"Blocked: {response.prompt_feedback.block_reason}\"\n",
        "                    }\n",
        "\n",
        "            # Extract answer\n",
        "            answer_text = response.text.strip()\n",
        "\n",
        "            # Extract cited sources from answer\n",
        "            cited_sources = []\n",
        "            for i, result in enumerate(contexts, start=1):\n",
        "                if f\"Source {i}\" in answer_text:\n",
        "                    cited_sources.append({\n",
        "                        'source_num': i,\n",
        "                        'page': result.chunk.page_number,\n",
        "                        'relevance': result.relevance_score,\n",
        "                        'text_preview': result.chunk.text[:200] + \"...\"\n",
        "                    })\n",
        "\n",
        "            return {\n",
        "                'answer': answer_text,\n",
        "                'sources': cited_sources,\n",
        "                'total_context_chunks': len(contexts),\n",
        "                'error': None\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            error_msg = str(e)\n",
        "\n",
        "            # Check if it's a rate limit error\n",
        "            if '429' in error_msg or 'quota' in error_msg.lower():\n",
        "                wait_time = 2 ** attempt  # Exponential backoff: 1s, 2s, 4s\n",
        "                print(f\"Rate limit hit. Waiting {wait_time}s... (Attempt {attempt+1}/{max_retries})\")\n",
        "                time.sleep(wait_time)\n",
        "                continue\n",
        "            else:\n",
        "                return {\n",
        "                    'answer': f\"Error generating answer: {error_msg}\",\n",
        "                    'sources': [],\n",
        "                    'error': error_msg\n",
        "                }\n",
        "\n",
        "    # All retries exhausted\n",
        "    return {\n",
        "        'answer': \"Could not generate answer due to rate limits. Please wait a moment and try again.\",\n",
        "        'sources': [],\n",
        "        'error': \"Max retries exceeded\"\n",
        "    }\n",
        "\n",
        "\n",
        "# Complete RAG Pipeline\n",
        "def answer_question(query: str, use_hybrid: bool = True) -> Dict[str, Any]:\n",
        "    print(f\"QUESTION: {query}\")\n",
        "\n",
        "    # Check if database has documents\n",
        "    if collection.count() == 0:\n",
        "        return {\n",
        "            'answer': \"No documents have been uploaded yet. Please upload a PDF first.\",\n",
        "            'sources': [],\n",
        "            'retrieval_results': [],\n",
        "            'error': 'No documents in database'\n",
        "        }\n",
        "\n",
        "    # Step 1: Retrieve relevant chunks\n",
        "    if use_hybrid:\n",
        "        retrieval_results = hybrid_search(query, top_k_total=8)\n",
        "    else:\n",
        "        retrieval_results = vector_search(query, top_k=5)\n",
        "\n",
        "    if not retrieval_results:\n",
        "        return {\n",
        "            'answer': \"Could not find relevant information in the document.\",\n",
        "            'sources': [],\n",
        "            'retrieval_results': [],\n",
        "            'error': 'No relevant chunks found'\n",
        "        }\n",
        "\n",
        "    # Filter by similarity threshold\n",
        "    filtered_results = [\n",
        "        r for r in retrieval_results\n",
        "        if r.relevance_score >= config.similarity_threshold\n",
        "    ]\n",
        "\n",
        "    if not filtered_results:\n",
        "        return {\n",
        "            'answer': f\"No highly relevant information found (threshold: {config.similarity_threshold}). The document may not contain information about this topic.\",\n",
        "            'sources': [],\n",
        "            'retrieval_results': retrieval_results,\n",
        "            'error': 'Below similarity threshold'\n",
        "        }\n",
        "\n",
        "    print(f\"\\nRetrieved {len(filtered_results)} relevant chunks:\")\n",
        "    for result in filtered_results[:3]:  # Show top 3\n",
        "        print(f\"   • {result}\")\n",
        "\n",
        "    # Step 2: Generate answer\n",
        "    print(f\"\\nGenerating answer with Gemini...\")\n",
        "    answer_data = generate_answer(query, filtered_results)\n",
        "\n",
        "    # Add retrieval results to response\n",
        "    answer_data['retrieval_results'] = filtered_results\n",
        "\n",
        "    if not answer_data.get('error'):\n",
        "        print(f\"Answer generated successfully!\")\n",
        "\n",
        "    return answer_data\n",
        "\n",
        "\n",
        "# Pretty Print Answer (for testing)\n",
        "def display_answer(result: Dict[str, Any]):\n",
        "    print(\"ANSWER:\")\n",
        "    print(result['answer'])\n",
        "\n",
        "    if result.get('sources'):\n",
        "        print(\"CITED SOURCES:\")\n",
        "        for source in result['sources']:\n",
        "            print(f\"\\n[Source {source['source_num']}]\")\n",
        "            print(f\"Page: {source['page']}\")\n",
        "            print(f\"Relevance: {source['relevance']:.2%}\")\n",
        "            print(f\"Preview: {source['text_preview']}\")\n",
        "\n",
        "print(\"Answer generation system initialized!\")\n",
        "print(\"Main function:\")\n",
        "print(\"• answer_question(query)\")\n",
        "print(\"• display_answer(result)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNxwMJLVzHce",
        "outputId": "666eed5b-f337-4f9b-9073-116115d2c91c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer generation system initialized!\n",
            "Main function:\n",
            "• answer_question(query)\n",
            "• display_answer(result)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Gradio Interface***"
      ],
      "metadata": {
        "id": "5ar2L4w60y6B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import json\n",
        "from datetime import datetime\n",
        "from typing import List, Tuple\n",
        "\n",
        "\n",
        "def upload_document(file) -> Tuple[str, str]:\n",
        "\n",
        "    if file is None:\n",
        "        return \"No file uploaded\", \"\"\n",
        "\n",
        "    try:\n",
        "        # Get file path\n",
        "        pdf_path = file.name\n",
        "\n",
        "        # Ingest document\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"UPLOADING: {Path(pdf_path).name}\")\n",
        "        print(f\"{'='*70}\")\n",
        "\n",
        "        stats = ingest_document(pdf_path)\n",
        "\n",
        "        # Create detailed status message\n",
        "        status = f\"\"\"\n",
        "**Document Uploaded Successfully!**\n",
        "\n",
        " **File:** {stats['filename']}\n",
        " **Document Type:** {stats['doc_type']}\n",
        " **Pages:** {stats['pages']}\n",
        " **Chunks Created:** {stats['chunks']}\n",
        " **Total Characters:** {stats['total_chars']:,}\n",
        " **Average Chunk Size:** {stats['avg_chunk_size']} chars\n",
        "\n",
        " **Database Status:** {collection.count()} total chunks stored\n",
        "\"\"\"\n",
        "\n",
        "        # Create stats JSON for display\n",
        "        stats_json = json.dumps(stats, indent=2)\n",
        "\n",
        "        return status, stats_json\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"\"\"\n",
        " **Upload Failed**\n",
        "\n",
        "**Error:** {str(e)}\n",
        "\n",
        "**Troubleshooting:**\n",
        "- Ensure the PDF is not corrupted\n",
        "- Check if the PDF contains extractable text\n",
        "- Try a different PDF file\n",
        "\"\"\"\n",
        "        return error_msg, \"\"\n",
        "\n",
        "\n",
        "def ask_question(question: str, use_hybrid: bool) -> Tuple[str, str, str]:\n",
        "    if not question or not question.strip():\n",
        "        return \"Please enter a question\", \"\", \"\"\n",
        "\n",
        "    if collection.count() == 0:\n",
        "        return \"Please upload a document first\", \"\", \"\"\n",
        "\n",
        "    try:\n",
        "        # Get answer\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"QUESTION: {question}\")\n",
        "        print(f\"{'='*70}\")\n",
        "\n",
        "        result = answer_question(question, use_hybrid=use_hybrid)\n",
        "\n",
        "        answer = f\"\"\"### Answer\n",
        "\n",
        "{result['answer']}\n",
        "\"\"\"\n",
        "        sources_text = \"\"\n",
        "        if result.get('sources'):\n",
        "            sources_text = \"### 📚 Sources\\n\\n\"\n",
        "            for source in result['sources']:\n",
        "                sources_text += f\"\"\"\n",
        "**Source {source['source_num']}** (Page {source['page']})\n",
        "\n",
        "- **Relevance:** {source['relevance']:.1%}\n",
        "- **Preview:** _{source['text_preview']}_\n",
        "\n",
        "---\n",
        "\n",
        "\"\"\"\n",
        "        else:\n",
        "            sources_text = \"### Sources\\n\\n_No specific sources cited in the answer._\"\n",
        "\n",
        "        retrieval_info = \"### Retrieval Details\\n\\n\"\n",
        "        if result.get('retrieval_results'):\n",
        "            retrieval_info += f\"**Total chunks retrieved:** {len(result['retrieval_results'])}\\n\\n\"\n",
        "\n",
        "            for i, res in enumerate(result['retrieval_results'][:5], 1):\n",
        "                retrieval_info += f\"\"\"\n",
        "**Chunk #{i}**\n",
        "\n",
        "- **Page:** {res.chunk.page_number}\n",
        "- **Type:** {res.chunk.doc_type}\n",
        "- **Score:** {res.relevance_score:.3f}\n",
        "- **Method:** {res.retrieval_method}\n",
        "\n",
        "\"\"\"\n",
        "        else:\n",
        "            retrieval_info += \"_No retrieval information available_\"\n",
        "\n",
        "        return answer, sources_text, retrieval_info\n",
        "\n",
        "    except Exception as e:\n",
        "        import traceback\n",
        "        error_details = traceback.format_exc()\n",
        "        print(f\"Error in ask_question: {error_details}\")\n",
        "\n",
        "        error_msg = f\"\"\"### Error\n",
        "\n",
        "**Message:** {str(e)}\n",
        "\n",
        "**Troubleshooting:**\n",
        "- Check if the document was uploaded successfully\n",
        "- Try rephrasing your question\n",
        "- Ensure Gemini API is working (check quota)\n",
        "\n",
        "**Details:** See console for full traceback\n",
        "\"\"\"\n",
        "        return error_msg, \"\", \"\"\n",
        "\n",
        "\n",
        "def get_database_stats() -> str:\n",
        "    \"\"\"Get current database statistics\"\"\"\n",
        "    try:\n",
        "        count = collection.count()\n",
        "        if count == 0:\n",
        "            return \"\"\"\n",
        "### Database Status\n",
        "\n",
        "**Status:** Empty\n",
        "**Chunks:** 0\n",
        "**Action:** Upload a document to get started!\n",
        "\"\"\"\n",
        "\n",
        "        # Get all documents\n",
        "        all_data = collection.get()\n",
        "\n",
        "        # Count by document type\n",
        "        from collections import Counter\n",
        "        doc_types = [meta.get('doc_type', 'unknown') for meta in all_data['metadatas']]\n",
        "        type_counts = Counter(doc_types)\n",
        "\n",
        "        # Count by page\n",
        "        pages = [meta.get('page_number', 0) for meta in all_data['metadatas']]\n",
        "        unique_pages = len(set(pages))\n",
        "\n",
        "        stats = f\"\"\"\n",
        "### Database Status\n",
        "\n",
        "**Total Chunks:** {count}\n",
        "**Unique Pages:** {unique_pages}\n",
        "\n",
        "**Document Types:**\n",
        "\"\"\"\n",
        "        for doc_type, cnt in type_counts.most_common():\n",
        "            stats += f\"- **{doc_type.title()}:** {cnt} chunks\\n\"\n",
        "\n",
        "        return stats\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error getting stats: {str(e)}\"\n",
        "\n",
        "\n",
        "def clear_database() -> str:\n",
        "    \"\"\"Clear all documents from database\"\"\"\n",
        "    try:\n",
        "        # Delete and recreate collection\n",
        "        chroma_client.delete_collection(name=config.collection_name)\n",
        "        global collection\n",
        "        collection = chroma_client.create_collection(\n",
        "            name=config.collection_name,\n",
        "            metadata={\"hnsw:space\": \"cosine\"}\n",
        "        )\n",
        "        return \"Database cleared successfully!\"\n",
        "    except Exception as e:\n",
        "        return f\"Error clearing database: {str(e)}\"\n",
        "\n",
        "# Custom CSS for styling\n",
        "custom_css = \"\"\"\n",
        ":root {\n",
        "    --primary-color: #6366f1;\n",
        "    --secondary-color: #8b5cf6;\n",
        "    --success-color: #10b981;\n",
        "    --warning-color: #f59e0b;\n",
        "    --error-color: #ef4444;\n",
        "}\n",
        "\n",
        ".markdown-text, .markdown-text * {\n",
        "    color: #e2e8f0 !important;\n",
        "}\n",
        "\n",
        ".markdown-text h1, .markdown-text h2, .markdown-text h3,\n",
        ".markdown-text h4, .markdown-text h5, .markdown-text h6 {\n",
        "    color: #a5b4fc !important;\n",
        "    font-weight: 600 !important;\n",
        "}\n",
        "\n",
        ".markdown-text strong {\n",
        "    color: #c7d2fe !important;\n",
        "}\n",
        "\n",
        ".markdown-text code {\n",
        "    background: #1e293b !important;\n",
        "    color: #fbbf24 !important;\n",
        "    padding: 2px 6px !important;\n",
        "    border-radius: 4px !important;\n",
        "}\n",
        "\n",
        ".info-box, .info-box * {\n",
        "    color: #1e293b !important;\n",
        "    background: #f1f5f9 !important;\n",
        "}\n",
        "\n",
        ".header-title {\n",
        "    background: linear-gradient(135deg, var(--primary-color), var(--secondary-color));\n",
        "    -webkit-background-clip: text;\n",
        "    -webkit-text-fill-color: transparent;\n",
        "    font-weight: 800;\n",
        "    font-size: 2.5rem;\n",
        "    text-align: center;\n",
        "    margin-bottom: 0.5rem;\n",
        "}\n",
        "\n",
        ".header-subtitle {\n",
        "    text-align: center;\n",
        "    color: #94a3b8;\n",
        "    font-size: 1.1rem;\n",
        "    margin-bottom: 2rem;\n",
        "}\n",
        "\n",
        ".gradio-container {\n",
        "    max-width: 1400px !important;\n",
        "    margin: auto;\n",
        "}\n",
        "\n",
        ".primary-btn {\n",
        "    background: linear-gradient(135deg, var(--primary-color), var(--secondary-color)) !important;\n",
        "    border: none !important;\n",
        "    color: white !important;\n",
        "    font-weight: 600 !important;\n",
        "    transition: transform 0.2s !important;\n",
        "}\n",
        "\n",
        ".primary-btn:hover {\n",
        "    transform: translateY(-2px) !important;\n",
        "    box-shadow: 0 10px 25px rgba(99, 102, 241, 0.3) !important;\n",
        "}\n",
        "\n",
        ".status-success {\n",
        "    color: var(--success-color);\n",
        "    font-weight: 600;\n",
        "}\n",
        "\n",
        ".status-error {\n",
        "    color: var(--error-color);\n",
        "    font-weight: 600;\n",
        "}\n",
        "\n",
        ".upload-area {\n",
        "    border: 2px dashed var(--primary-color);\n",
        "    border-radius: 1rem;\n",
        "    padding: 2rem;\n",
        "    text-align: center;\n",
        "    transition: all 0.3s;\n",
        "}\n",
        "\n",
        ".upload-area:hover {\n",
        "    border-color: var(--secondary-color);\n",
        "    background: #1e1b4b;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# Build the interface\n",
        "with gr.Blocks(\n",
        "    css=custom_css,\n",
        "    title=\"RAG Document Q&A System\",\n",
        "    theme=gr.themes.Soft(\n",
        "        primary_hue=\"indigo\",\n",
        "        secondary_hue=\"purple\",\n",
        "        neutral_hue=\"slate\",\n",
        "        font=gr.themes.GoogleFont(\"Inter\")\n",
        "    )\n",
        ") as demo:\n",
        "\n",
        "    # Header\n",
        "    gr.HTML(\"\"\"\n",
        "        <div style='text-align: center; padding: 2rem 0;'>\n",
        "            <h1 class='header-title'>Advanced RAG Document Q&A System</h1>\n",
        "            <p class='header-subtitle'>\n",
        "                Upload PDFs • Extract Information • Get AI-Powered Answers\n",
        "            </p>\n",
        "        </div>\n",
        "    \"\"\")\n",
        "\n",
        "    # Main content in tabs\n",
        "    with gr.Tabs() as tabs:\n",
        "\n",
        "        with gr.Tab(\"📤 Upload Document\", id=0):\n",
        "            gr.Markdown(\"\"\"\n",
        "            ### Upload Your PDF\n",
        "            Supported document types: **Resumes, Invoices, Contracts, Payslips, Reports, and more**\n",
        "            \"\"\")\n",
        "\n",
        "            with gr.Row():\n",
        "                with gr.Column(scale=2):\n",
        "                    file_input = gr.File(\n",
        "                        label=\"Choose PDF File\",\n",
        "                        file_types=[\".pdf\"],\n",
        "                        type=\"filepath\"\n",
        "                    )\n",
        "\n",
        "                    upload_btn = gr.Button(\n",
        "                        \"Upload & Process\",\n",
        "                        variant=\"primary\",\n",
        "                        size=\"lg\",\n",
        "                        elem_classes=[\"primary-btn\"]\n",
        "                    )\n",
        "\n",
        "                    upload_status = gr.Markdown(\n",
        "                        value=\"*Upload a document to begin*\",\n",
        "                        elem_classes=[\"info-box\"]\n",
        "                    )\n",
        "\n",
        "                with gr.Column(scale=1):\n",
        "                    gr.Markdown(\"### 📊 Document Stats\")\n",
        "                    stats_json = gr.JSON(\n",
        "                        label=\"Ingestion Details\",\n",
        "                        value={}\n",
        "                    )\n",
        "\n",
        "            # Processing info\n",
        "            gr.Markdown(\"\"\"\n",
        "            ---\n",
        "            ### What happens during processing?\n",
        "\n",
        "            1. **Text Extraction** - Advanced parsing with Docling\n",
        "            2. **Document Classification** - Auto-detect document type per page\n",
        "            3. **Smart Chunking** - Intelligent text segmentation\n",
        "            4. **Embedding Generation** - Create vector representations\n",
        "            5. **Vector Storage** - Store in ChromaDB for fast retrieval\n",
        "            \"\"\")\n",
        "\n",
        "\n",
        "        with gr.Tab(\"❓ Ask Questions\", id=1):\n",
        "            gr.Markdown(\"\"\"\n",
        "            ### Ask Questions About Your Documents\n",
        "            Get AI-powered answers with source citations\n",
        "            \"\"\")\n",
        "\n",
        "            with gr.Row():\n",
        "                with gr.Column(scale=3):\n",
        "                    question_input = gr.Textbox(\n",
        "                        label=\"Your Question\",\n",
        "                        placeholder=\"e.g., What is John Smith's educational background?\",\n",
        "                        lines=2,\n",
        "                        max_lines=5\n",
        "                    )\n",
        "\n",
        "                    with gr.Row():\n",
        "                        hybrid_toggle = gr.Checkbox(\n",
        "                            label=\"Use Hybrid Search (Vector + KNN)\",\n",
        "                            value=True,\n",
        "                            info=\"Combines semantic and keyword-based retrieval for better results\"\n",
        "                        )\n",
        "\n",
        "                    ask_btn = gr.Button(\n",
        "                        \"Get Answer\",\n",
        "                        variant=\"primary\",\n",
        "                        size=\"lg\",\n",
        "                        elem_classes=[\"primary-btn\"]\n",
        "                    )\n",
        "\n",
        "                with gr.Column(scale=1):\n",
        "                    gr.Markdown(\"### 💡 Example Questions\")\n",
        "                    example_questions = [\n",
        "                        \"What is the candidate's work experience?\",\n",
        "                        \"What is the total amount on the invoice?\",\n",
        "                        \"What are the employee's deductions?\",\n",
        "                        \"When does the contract expire?\",\n",
        "                        \"What are the key skills mentioned?\"\n",
        "                    ]\n",
        "\n",
        "                    for eq in example_questions:\n",
        "                        gr.Button(\n",
        "                            eq,\n",
        "                            size=\"sm\",\n",
        "                            variant=\"secondary\"\n",
        "                        ).click(\n",
        "                            fn=lambda x: x,\n",
        "                            inputs=gr.State(eq),\n",
        "                            outputs=question_input\n",
        "                        )\n",
        "\n",
        "            # Answer display\n",
        "            gr.Markdown(\"### Response\")\n",
        "\n",
        "            with gr.Row():\n",
        "                with gr.Column(scale=2):\n",
        "                    answer_output = gr.Markdown(\n",
        "                        label=\"Answer\",\n",
        "                        elem_classes=[\"markdown-text\"]\n",
        "                    )\n",
        "\n",
        "                with gr.Column(scale=1):\n",
        "                    sources_output = gr.Markdown(\n",
        "                        label=\"Sources\",\n",
        "                        elem_classes=[\"markdown-text\"]\n",
        "                    )\n",
        "\n",
        "            # Retrieval details (collapsible)\n",
        "            with gr.Accordion(\"Retrieval Details\", open=False):\n",
        "                retrieval_output = gr.Markdown()\n",
        "\n",
        "\n",
        "        with gr.Tab(\"💾 Database\", id=2):\n",
        "            gr.Markdown(\"\"\"\n",
        "            ### Database Management\n",
        "            View and manage your document database\n",
        "            \"\"\")\n",
        "\n",
        "            with gr.Row():\n",
        "                with gr.Column():\n",
        "                    db_stats = gr.Markdown(\n",
        "                        value=get_database_stats()\n",
        "                    )\n",
        "\n",
        "                    with gr.Row():\n",
        "                        refresh_btn = gr.Button(\n",
        "                            \"Refresh Stats\",\n",
        "                            variant=\"secondary\"\n",
        "                        )\n",
        "\n",
        "                        clear_btn = gr.Button(\n",
        "                            \"Clear Database\",\n",
        "                            variant=\"stop\"\n",
        "                        )\n",
        "\n",
        "                    clear_status = gr.Markdown()\n",
        "\n",
        "                with gr.Column():\n",
        "                    gr.Markdown(\"\"\"\n",
        "                    ### System Configuration\n",
        "\n",
        "                    **Embedding Model:** `all-MiniLM-L6-v2`\n",
        "                    - **Dimensions:** 384\n",
        "                    - **Size:** 22M parameters\n",
        "\n",
        "                    **LLM Model:** `gemini-2.0-flash-exp`\n",
        "                    - **Provider:** Google Gemini\n",
        "                    - **Temperature:** 0.3\n",
        "\n",
        "                    **Chunking Strategy:**\n",
        "                    - **Chunk Size:** 800 chars\n",
        "                    - **Overlap:** 150 chars\n",
        "\n",
        "                    **Retrieval:**\n",
        "                    - **Vector Search:** Top 5\n",
        "                    - **KNN Search:** Top 3\n",
        "                    - **Similarity Threshold:** 0.3\n",
        "                    \"\"\")\n",
        "\n",
        "\n",
        "        with gr.Tab(\"ℹ️ About\", id=3):\n",
        "            gr.Markdown(\"\"\"\n",
        "            # Advanced RAG Document Q&A System\n",
        "\n",
        "            ## Features\n",
        "\n",
        "            ### Document Processing\n",
        "            - **Multi-format support** - PDFs with text, tables, and forms\n",
        "            - **Per-page classification** - Automatically detect document types (resume, invoice, payslip, etc.)\n",
        "            - **Smart chunking** - Context-aware text segmentation with overlap\n",
        "\n",
        "            ### Hybrid Retrieval System\n",
        "            - **Vector Search** - Semantic similarity using sentence embeddings\n",
        "            - **KNN Search** - Keyword-based TF-IDF retrieval for diversity\n",
        "            - **Combined Results** - Best of both worlds for accurate retrieval\n",
        "\n",
        "            ### AI-Powered Answers\n",
        "            - **Google Gemini 2.0** - State-of-the-art language model\n",
        "            - **Source Citations** - Every answer includes references\n",
        "            - **Context-aware** - Uses only relevant document chunks\n",
        "\n",
        "            ### Vector Database\n",
        "            - **ChromaDB** - Fast, persistent vector storage\n",
        "            - **Cosine Similarity** - Optimized for semantic search\n",
        "            - **Scalable** - Handles multiple documents efficiently\n",
        "\n",
        "            ## Technology Stack\n",
        "\n",
        "            - **Document Parsing:** Docling (with PyPDF2 fallback)\n",
        "            - **Embeddings:** SentenceTransformers (all-MiniLM-L6-v2)\n",
        "            - **Vector Store:** ChromaDB\n",
        "            - **LLM:** Google Gemini 2.0 Flash\n",
        "            - **UI:** Gradio\n",
        "            - **Search:** Hybrid (Vector + KNN)\n",
        "\n",
        "            ## Supported Document Types\n",
        "\n",
        "            -  Resumes / CVs\n",
        "            -  Invoices / Bills\n",
        "            -  Payslips\n",
        "            -  Contracts\n",
        "            -  Financial Statements\n",
        "            -  Reports\n",
        "            -  General Documents\n",
        "\n",
        "            ##  How It Works\n",
        "\n",
        "            1. **Upload** - Drop your PDF into the upload tab\n",
        "            2. **Process** - System extracts text, classifies pages, creates embeddings\n",
        "            3. **Store** - Chunks saved in vector database with metadata\n",
        "            4. **Ask** - Type your question in natural language\n",
        "            5. **Retrieve** - Hybrid search finds most relevant chunks\n",
        "            6. **Answer** - Gemini generates accurate response with sources\n",
        "\n",
        "            ##  Tips for Best Results\n",
        "\n",
        "            - Use clear, specific questions\n",
        "            - For multi-document PDFs, ask about specific sections\n",
        "            - Enable hybrid search for complex queries\n",
        "            - Check source citations to verify answers\n",
        "\n",
        "            ---\n",
        "\n",
        "            **Version:** 1.0.0 | **Last Updated:** January 2026\n",
        "            \"\"\")\n",
        "\n",
        "\n",
        "    # Upload button\n",
        "    upload_btn.click(\n",
        "        fn=upload_document,\n",
        "        inputs=[file_input],\n",
        "        outputs=[upload_status, stats_json]\n",
        "    ).then(\n",
        "        fn=get_database_stats,\n",
        "        outputs=[db_stats]\n",
        "    )\n",
        "\n",
        "    # Ask button\n",
        "    ask_btn.click(\n",
        "        fn=ask_question,\n",
        "        inputs=[question_input, hybrid_toggle],\n",
        "        outputs=[answer_output, sources_output, retrieval_output]\n",
        "    )\n",
        "\n",
        "    # Refresh database stats\n",
        "    refresh_btn.click(\n",
        "        fn=get_database_stats,\n",
        "        outputs=[db_stats]\n",
        "    )\n",
        "\n",
        "    # Clear database\n",
        "    clear_btn.click(\n",
        "        fn=clear_database,\n",
        "        outputs=[clear_status]\n",
        "    ).then(\n",
        "        fn=get_database_stats,\n",
        "        outputs=[db_stats]\n",
        "    )\n",
        "\n",
        "\n",
        "print(\" LAUNCHING GRADIO UI\")\n",
        "\n",
        "# Close any existing Gradio instances\n",
        "try:\n",
        "    gr.close_all()\n",
        "    print(\"Closed previous Gradio instances\")\n",
        "except:\n",
        "    pass\n",
        "demo.launch(\n",
        "    share=True,\n",
        "    debug=True,\n",
        "    show_error=True,\n",
        "    server_name=\"0.0.0.0\",\n",
        "    inline=False\n",
        ")\n",
        "\n",
        "print(\"\\n UI is live!\")\n",
        "print(\"Use the public URL to access from anywhere\")\n",
        "print(\"Session will expire after inactivity\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "c29660aab9f54d76b3323a33c20a2382",
            "af371c6d8ef34b29a56b2f4801542c54",
            "1e20e050c4d940f29e3d0596f6a99177",
            "eddf0324af234ecda8981954b13f65a6",
            "acc6763fea884027ac8a43954d713ac5",
            "5b3f8ac3ee354b1ca6c13dc49d43c479",
            "d312016948c64e5785fcf0b1c5d160b3",
            "d74140f64cf341039bcaf8e1574dc55e",
            "4b4f3c36c2ea441b924cd6ebd3172871",
            "4bba18f9dc3d487d93d5b98b527af46e",
            "188b7f73477a4738aba0329392868655"
          ]
        },
        "id": "H0Ipe-mx7YEA",
        "outputId": "6afcc7c2-6a70-41c8-82db-76b5d68b4f91"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " LAUNCHING GRADIO UI\n",
            "Closing server running on port: 7860\n",
            "Closed previous Gradio instances\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://e00ee4f2191c3f5e64.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:    Exception in ASGI application\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/protocols/http/httptools_impl.py\", line 409, in run_asgi\n",
            "    result = await app(  # type: ignore[func-returns-value]\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/middleware/proxy_headers.py\", line 60, in __call__\n",
            "    return await self.app(scope, receive, send)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/applications.py\", line 1139, in __call__\n",
            "    await super().__call__(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/applications.py\", line 107, in __call__\n",
            "    await self.middleware_stack(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/middleware/errors.py\", line 186, in __call__\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/middleware/errors.py\", line 164, in __call__\n",
            "    await self.app(scope, receive, _send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/brotli_middleware.py\", line 74, in __call__\n",
            "    return await self.app(scope, receive, send)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/route_utils.py\", line 882, in __call__\n",
            "    await self.app(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/middleware/exceptions.py\", line 63, in __call__\n",
            "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n",
            "    await app(scope, receive, sender)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/middleware/asyncexitstack.py\", line 18, in __call__\n",
            "    await self.app(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/routing.py\", line 716, in __call__\n",
            "    await self.middleware_stack(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/routing.py\", line 736, in app\n",
            "    await route.handle(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/routing.py\", line 290, in handle\n",
            "    await self.app(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/routing.py\", line 119, in app\n",
            "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n",
            "    await app(scope, receive, sender)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/routing.py\", line 105, in app\n",
            "    response = await f(request)\n",
            "               ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/routing.py\", line 385, in app\n",
            "    raw_response = await run_endpoint_function(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/routing.py\", line 284, in run_endpoint_function\n",
            "    return await dependant.call(**values)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/routes.py\", line 1671, in get_upload_progress\n",
            "    await asyncio.wait_for(\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 520, in wait_for\n",
            "    return await fut\n",
            "           ^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/route_utils.py\", line 528, in is_tracked\n",
            "    return await self._signals[upload_id].wait()\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/locks.py\", line 209, in wait\n",
            "    fut = self._get_loop().create_future()\n",
            "          ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/mixins.py\", line 20, in _get_loop\n",
            "    raise RuntimeError(f'{self!r} is bound to a different event loop')\n",
            "RuntimeError: <asyncio.locks.Event object at 0x7b78861fcc80 [unset]> is bound to a different event loop\n",
            "\u001b[32m[INFO] 2026-01-05 08:55:07,658 [RapidOCR] base.py:22: Using engine_name: onnxruntime\u001b[0m\n",
            "\u001b[32m[INFO] 2026-01-05 08:55:07,672 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/lib/python3.12/dist-packages/rapidocr/models/ch_PP-OCRv4_det_infer.onnx\u001b[0m\n",
            "\u001b[32m[INFO] 2026-01-05 08:55:07,673 [RapidOCR] main.py:53: Using /usr/local/lib/python3.12/dist-packages/rapidocr/models/ch_PP-OCRv4_det_infer.onnx\u001b[0m\n",
            "\u001b[32m[INFO] 2026-01-05 08:55:07,743 [RapidOCR] base.py:22: Using engine_name: onnxruntime\u001b[0m\n",
            "\u001b[32m[INFO] 2026-01-05 08:55:07,747 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/lib/python3.12/dist-packages/rapidocr/models/ch_ppocr_mobile_v2.0_cls_infer.onnx\u001b[0m\n",
            "\u001b[32m[INFO] 2026-01-05 08:55:07,748 [RapidOCR] main.py:53: Using /usr/local/lib/python3.12/dist-packages/rapidocr/models/ch_ppocr_mobile_v2.0_cls_infer.onnx\u001b[0m\n",
            "\u001b[32m[INFO] 2026-01-05 08:55:07,784 [RapidOCR] base.py:22: Using engine_name: onnxruntime\u001b[0m\n",
            "\u001b[32m[INFO] 2026-01-05 08:55:07,816 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/lib/python3.12/dist-packages/rapidocr/models/ch_PP-OCRv4_rec_infer.onnx\u001b[0m\n",
            "\u001b[32m[INFO] 2026-01-05 08:55:07,816 [RapidOCR] main.py:53: Using /usr/local/lib/python3.12/dist-packages/rapidocr/models/ch_PP-OCRv4_rec_infer.onnx\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "UPLOADING: Blob File Sample.pdf\n",
            "======================================================================\n",
            "INGESTING DOCUMENT: Blob File Sample.pdf\n",
            "Processing PDF with Docling: Blob File Sample.pdf\n",
            "Converting document...\n",
            "Exported as Markdown\n",
            "Per-page extraction failed, splitting full text...\n",
            "Extracted 4 pages\n",
            "Total characters: 10,255\n",
            "Page sizes: [2563, 2552, 2544, 2566]\n",
            "Per-page types: ['resume', 'general', 'invoice', 'invoice']\n",
            "Overall type: invoice (most common)\n",
            "\n",
            "Chunking 4 pages...\n",
            "Page 1 (invoice): 4 chunks (2530 chars)\n",
            "Page 1: 4 chunks\n",
            "Page 2 (invoice): 4 chunks (2552 chars)\n",
            "Page 2: 4 chunks\n",
            "Page 3 (invoice): 4 chunks (2540 chars)\n",
            "Page 3: 4 chunks\n",
            "Page 4 (invoice): 4 chunks (2531 chars)\n",
            "Page 4: 4 chunks\n",
            "\n",
            "Created 16 total chunks\n",
            "Generating embeddings...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c29660aab9f54d76b3323a33c20a2382"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated 16 embeddings\n",
            "Storing in vector database...\n",
            "\n",
            "Document ingested successfully!\n",
            "Total chunks in database: 21\n",
            "\n",
            "======================================================================\n",
            "QUESTION: What degrees did John W. Smith earn and what were his GPAs?\n",
            "======================================================================\n",
            "QUESTION: What degrees did John W. Smith earn and what were his GPAs?\n",
            "\n",
            "Performing hybrid search for: 'What degrees did John W. Smith earn and what were his GPAs?...'\n",
            "Vector search: 5 results\n",
            "KNN search: 3 results\n",
            "Combined: 5 unique results\n",
            "\n",
            "Retrieved 5 relevant chunks:\n",
            "   • Rank 1 | Score: 0.659 | Method: vector | Page: 1\n",
            "   • Rank 2 | Score: 0.632 | Method: vector | Page: 1\n",
            "   • Rank 3 | Score: 0.605 | Method: vector | Page: 1\n",
            "\n",
            "Generating answer with Gemini...\n",
            "Answer generated successfully!\n",
            "\n",
            "======================================================================\n",
            "QUESTION: What is John Smith's educational background?\n",
            "======================================================================\n",
            "QUESTION: What is John Smith's educational background?\n",
            "\n",
            "Performing hybrid search for: 'What is John Smith's educational background?...'\n",
            "Vector search: 5 results\n",
            "KNN search: 3 results\n",
            "Combined: 5 unique results\n",
            "\n",
            "Retrieved 5 relevant chunks:\n",
            "   • Rank 1 | Score: 0.658 | Method: vector | Page: 1\n",
            "   • Rank 2 | Score: 0.656 | Method: vector | Page: 1\n",
            "   • Rank 3 | Score: 0.617 | Method: vector | Page: 1\n",
            "\n",
            "Generating answer with Gemini...\n",
            "Answer generated successfully!\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 0.0.0.0:7860 <> https://e00ee4f2191c3f5e64.gradio.live\n",
            "\n",
            " UI is live!\n",
            "Use the public URL to access from anywhere\n",
            "Session will expire after inactivity\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dkV0rid3X7IS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}